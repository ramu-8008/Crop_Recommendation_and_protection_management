{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7af66297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "# Define hyperparameters for each model for grid search\n",
    "param_grid = {\n",
    "    'LogisticRegression': {'max_iter': [1000]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'DecisionTreeClassifier': {'max_depth': [None, 5, 10, 15]},\n",
    "    'KNeighborsClassifier': {'n_neighbors': [3, 5, 7]},\n",
    "    'GaussianNB': {},  # No hyperparameters for GaussianNB\n",
    "    'RandomForestClassifier': {'n_estimators': [50, 100, 200]},\n",
    "    'VotingClassifier': {},  # Hyperparameters are set inside VotingClassifier definition\n",
    "    'BaggingClassifier': {'n_estimators': [10, 50, 100]},\n",
    "    'AdaBoostClassifier': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]},\n",
    "    'GradientBoostingClassifier': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]}\n",
    "}\n",
    "\n",
    "# List of models with their respective names\n",
    "models = [\n",
    "    ('LogisticRegression', LogisticRegression()),\n",
    "    ('SVM', SVC()),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier()),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('RandomForestClassifier', RandomForestClassifier()),\n",
    "    ('VotingClassifier', VotingClassifier([('Random Forest', RandomForestClassifier()),\n",
    "                                           ('SVM', SVC()),\n",
    "                                           ('Logistic Regression', LogisticRegression(max_iter=1000))])),\n",
    "    ('BaggingClassifier', BaggingClassifier()),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier()),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "731e85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cbfaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegression', LogisticRegression()), ('DecisionTreeClassifier', DecisionTreeClassifier()), ('KNeighborsClassifier', KNeighborsClassifier()), ('GaussianNB', GaussianNB()), ('RandomForestClassifier', RandomForestClassifier()), ('BaggingClassifier', BaggingClassifier()), ('AdaBoostClassifier', AdaBoostClassifier()), ('GradientBoostingClassifier', GradientBoostingClassifier())]\n"
     ]
    }
   ],
   "source": [
    "# List to store models supporting predict_proba\n",
    "models_with_proba = []\n",
    "\n",
    "# Check and add models supporting predict_proba to the list\n",
    "for name, model in models:\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        models_with_proba.append((name, model))\n",
    "\n",
    "# Access models_with_proba for further use\n",
    "print(models_with_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8109ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92f9d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "crops = ['apple', 'arecanut', 'ashgourd', 'banana', 'barley', 'beetroot',\n",
    "       'bittergourd', 'blackgram', 'blackpepper', 'bottlegourd',\n",
    "       'brinjal', 'cabbage', 'cardamom', 'carrot', 'cashewnuts',\n",
    "       'cauliflower', 'coffee', 'coriander', 'cotton', 'cucumber',\n",
    "       'drumstick', 'garlic', 'ginger', 'grapes', 'horsegram',\n",
    "       'jackfruit', 'jowar', 'jute', 'ladyfinger', 'maize', 'mango',\n",
    "       'moong', 'onion', 'orange', 'papaya', 'pineapple', 'pomegranate',\n",
    "       'potato', 'pumpkin', 'radish', 'ragi', 'rapeseed', 'rice',\n",
    "       'ridgegourd', 'sesamum', 'soyabean', 'sunflower', 'sweetpotato',\n",
    "       'tapioca', 'tomato', 'turmeric', 'watermelon', 'wheat']\n",
    "states = ['andaman and nicobar islands', 'andhra pradesh',\n",
    "       'arunachal pradesh', 'assam', 'bihar', 'chandigarh',\n",
    "       'chhattisgarh', 'dadra and nagar haveli', 'goa', 'gujarat',\n",
    "       'haryana', 'himachal pradesh', 'jammu and kashmir', 'jharkhand',\n",
    "       'karnataka', 'kerala', 'madhya pradesh', 'maharashtra', 'manipur',\n",
    "       'meghalaya', 'mizoram', 'nagaland', 'odisha', 'puducherry',\n",
    "       'punjab', 'rajasthan', 'sikkim', 'tamil nadu', 'telangana',\n",
    "       'tripura', 'uttar pradesh', 'uttarakhand', 'west bengal']\n",
    "new_scaler = joblib.load('scalerV1.0.pkl')\n",
    "# Create a dictionary to store the total probabilities for each crop\n",
    "crop_probabilities = {crop: 0.0 for crop in crops}\n",
    "\n",
    "def ml_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "    probs = (sum(probs)/len(models_with_proba))*100\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs)])\n",
    "            \n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs)])\n",
    "        print(\"accuracy : \", max(probs))\n",
    "        i = np.argmax(probs)\n",
    "        probs[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafb156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00cff715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Probable Crop: onion\n",
      "accuracy :  89.74243443419874\n",
      "optional crop  1  : cabbage\n",
      "accuracy :  2.104515749497476\n",
      "optional crop  2  : pineapple\n",
      "accuracy :  1.4394212770760482\n",
      "optional crop  3  : tomato\n",
      "accuracy :  1.4171270261571236\n",
      "optional crop  4  : beetroot\n",
      "accuracy :  1.1275906755351057\n"
     ]
    }
   ],
   "source": [
    "new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]#[\"jammu and kashmir\",60,30,30,6.11,293.36,14.700000]\n",
    "#states.index(new_data[0])\n",
    "ml_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e434c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "#import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Load the model\n",
    "# model = tf.keras.models.load_model('path_to_your_model.keras')\n",
    "\n",
    "# Use the model for inference or any required operations\n",
    "# For example:\n",
    "# result = model.predict(input_data)\n",
    "\n",
    "def data_for_cnn(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    return new_data1\n",
    "def data_for_ann(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    return new_data1\n",
    "def data_for_LSTM(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = np.array(new_scaler.transform([new_data1]))\n",
    "    new_data1 = new_data1.reshape(new_data1.shape[0], 1,new_data1.shape[1])\n",
    "    return new_data1\n",
    "def data_for_voting_dl(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    new_data1 = [new_data1,new_data1]\n",
    "    return new_data1\n",
    "def prediction_from_ann(new_data):\n",
    "    ann_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/ANN_V10.keras\")\n",
    "    pred = ann_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "def prediction_from_cnn(new_data):\n",
    "    cnn_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/CNN_V10.keras\")\n",
    "    pred = cnn_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "def prediction_from_lstm(new_data):\n",
    "    lstm_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/LSTM_V11.keras\")\n",
    "    pred = lstm_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "\n",
    "\n",
    "import joblib\n",
    "def create_lstm_model():\n",
    "    LSTM_V10 = Sequential()\n",
    "    LSTM_V10.add(LSTM(256, input_shape=(1, X_train_reshaped.shape[2]), activation='relu'))\n",
    "    #model.add(Dense(128,activation = 'relu'))\n",
    "    LSTM_V10.add(Dense(53, activation='softmax'))  # Adjust output units and activation for multiclass\n",
    "    # Compile the model\n",
    "    LSTM_V10.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return LSTM_V10\n",
    "\n",
    "\n",
    "\n",
    "# Function to create GRU model\n",
    "def create_gru_model():\n",
    "    GRU_V10 = Sequential()\n",
    "    GRU_V10.add(GRU(256, input_shape=(1, X_train_reshaped.shape[2]), kernel_initializer=he_normal(), activation=LeakyReLU(alpha=0.03), return_sequences=True))\n",
    "    GRU_V10.add(GRU(128, activation=LeakyReLU(alpha=0.03)))  # Additional GRU layer\n",
    "    #model2.add(Dense(64, activation=LeakyReLU(alpha=0.03)))\n",
    "    GRU_V10.add(Dense(53, activation='sigmoid'))\n",
    "    GRU_V10.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.03), metrics=['accuracy'])\n",
    "    return GRU_V10\n",
    "def create_ann_model():\n",
    "    ANN_V10 = Sequential()\n",
    "    # Add layers\n",
    "    ANN_V10.add(Dense(128, input_shape=(1, X_train_reshaped.shape[2]), activation='relu'))\n",
    "    ANN_V10.add(Dense(64, activation='relu'))\n",
    "    ANN_V10.add(Dense(32, activation='relu'))\n",
    "    ANN_V10.add(Dense(53, activation='softmax'))  # Softmax for multi-class classification\n",
    "    # Compile the model\n",
    "    ANN_V10.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return ANN_V10\n",
    "voting_classifier = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/voting_classifier_100epV12.pkl')\n",
    "def prediction_from_voting_dl(new_data):\n",
    "    pred = voting_classifier.predict_proba(new_data)\n",
    "    return crops[np.argmax(pred)],pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06b6ba73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_for_ann_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_for_ann_cnn\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m120\u001b[39m,\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m65\u001b[39m,\u001b[38;5;241m6.12\u001b[39m,\u001b[38;5;241m2169.32\u001b[39m,\u001b[38;5;241m23.736364\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommended Crop from ANN: \u001b[39m\u001b[38;5;124m\"\u001b[39m,prediction_from_ann(new_data))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_for_ann_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "new_data = data_for_ann_cnn([\"assam\",120,60,65,6.12,2169.32,23.736364])\n",
    "print(\"recommended Crop from ANN: \",prediction_from_ann(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c9464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "recommended Crop from CNN:  ('rice', array([2.5004547e-11, 2.6155769e-10, 2.1820767e-10, 8.0795003e-11,\n",
      "       5.6179733e-10, 1.4495841e-10, 1.6460579e-12, 9.2086908e-14,\n",
      "       2.2143848e-11, 1.2771957e-10, 1.8981455e-12, 5.7489235e-12,\n",
      "       3.1000123e-11, 4.7591327e-11, 1.4100859e-10, 1.4650741e-11,\n",
      "       5.9005446e-11, 2.7829848e-12, 1.8443697e-12, 5.4013768e-12,\n",
      "       2.3741922e-12, 2.7415037e-10, 1.8919144e-12, 7.4618631e-11,\n",
      "       6.4754679e-10, 1.4560244e-09, 3.6667209e-02, 2.9211588e-08,\n",
      "       7.0449649e-12, 1.3284528e-11, 1.6714637e-11, 1.3886502e-11,\n",
      "       2.2742087e-10, 2.1907300e-10, 1.5453658e-10, 6.9288172e-11,\n",
      "       1.3565262e-11, 2.4073360e-10, 8.0644684e-12, 5.4409553e-13,\n",
      "       7.1569618e-11, 3.8780878e-11, 9.6333283e-01, 3.2670500e-10,\n",
      "       2.5754956e-12, 7.2535318e-11, 3.7648992e-11, 7.5072470e-10,\n",
      "       7.5899764e-10, 1.0853918e-11, 6.7225350e-11, 3.3937425e-10,\n",
      "       1.6040589e-11], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "new_data = data_for_ann_cnn([\"jammu and kashmir\",80,40,40,5.38,516.68,27.866667])\n",
    "print(\"recommended Crop from CNN: \",prediction_from_cnn(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F80E58B010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "recommended Crop from LSTM:  ('rice', array([2.68734479e-11, 5.84806578e-11, 3.05400184e-11, 5.83367243e-11,\n",
      "       2.91876461e-08, 2.31312122e-11, 1.01537737e-11, 1.24706981e-10,\n",
      "       4.63489143e-11, 1.64341769e-11, 1.77559905e-08, 3.07661605e-11,\n",
      "       2.88759086e-11, 6.48599716e-11, 3.10948801e-11, 6.95363073e-11,\n",
      "       8.31392941e-12, 6.06675335e-11, 1.60989462e-08, 4.48545853e-11,\n",
      "       3.53082770e-13, 1.80632592e-11, 5.51819770e-11, 2.99737804e-11,\n",
      "       1.64477348e-10, 9.19871059e-13, 3.41701172e-02, 7.09909784e-08,\n",
      "       1.92762872e-09, 5.19333376e-09, 1.66196015e-10, 5.51064576e-11,\n",
      "       2.89142668e-11, 1.64649974e-10, 1.55193167e-11, 2.77978821e-11,\n",
      "       1.02879354e-11, 2.00870136e-11, 2.42288203e-11, 3.16101034e-11,\n",
      "       4.60655042e-10, 8.93283281e-10, 9.65829730e-01, 4.82603714e-11,\n",
      "       2.76618485e-11, 4.81015713e-11, 9.10776177e-11, 3.76091727e-11,\n",
      "       1.43613463e-11, 2.60656123e-12, 1.98858100e-11, 3.29748256e-10,\n",
      "       4.84411511e-10], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "new_data = data_for_LSTM([\"jammu and kashmir\",80,40,40,5.38,516.68,27.866667])\n",
    "print(\"recommended Crop from LSTM: \",prediction_from_lstm(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21978ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "recommended Crop from Voting classifier:  ('rice', array([7.92979848e-11, 1.70989334e-10, 5.90141269e-11, 8.78175241e-11,\n",
      "       2.49416416e-08, 7.50913429e-11, 4.40511690e-11, 2.83588014e-10,\n",
      "       8.10998768e-11, 3.74449395e-11, 8.06657408e-09, 5.03140030e-11,\n",
      "       8.39826819e-11, 7.39921110e-11, 1.13312450e-11, 1.35479530e-10,\n",
      "       4.67739181e-11, 4.15272261e-11, 9.23561727e-10, 4.11365109e-11,\n",
      "       1.25679632e-12, 1.18526203e-10, 3.83863809e-11, 9.73897074e-11,\n",
      "       1.21885294e-10, 1.40285075e-11, 3.04153692e-02, 2.05002425e-06,\n",
      "       8.66774297e-09, 2.12092732e-10, 6.71339720e-11, 3.74911525e-11,\n",
      "       1.07083863e-10, 5.26049725e-11, 7.08363299e-11, 3.37652753e-11,\n",
      "       7.45031745e-11, 6.28631591e-11, 2.23571977e-11, 1.05378616e-10,\n",
      "       6.48694570e-11, 9.10662823e-10, 9.48983729e-01, 2.62078258e-11,\n",
      "       9.04323352e-11, 4.78354682e-11, 1.00774451e-10, 8.38164260e-11,\n",
      "       3.16743472e-11, 1.57232023e-11, 7.26149627e-11, 8.11467948e-10,\n",
      "       8.89804397e-10], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "new_data = data_for_voting_dl([\"jammu and kashmir\",80,40,40,5.38,516.68,27.866667])\n",
    "print(\"recommended Crop from Voting classifier: \",prediction_from_voting_dl(new_data))\n",
    "#crops[voting_classifier.predict(new_data)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5bac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_models_with_proba = [(data_for_ann,prediction_from_ann),(data_for_cnn,prediction_from_cnn),\n",
    "                        (data_for_LSTM,prediction_from_lstm),(data_for_voting_dl,prediction_from_voting_dl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_ann_cnn(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    return new_data1\n",
    "def data_for_LSTM(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = np.array(new_scaler.transform([new_data1]))\n",
    "    new_data1 = new_data1.reshape(new_data1.shape[0], 1,new_data1.shape[1])\n",
    "    return new_data1\n",
    "def data_for_voting_dl(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    new_data1 = [new_data1,new_data1]\n",
    "    return new_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Most Probable Crop: onion\n",
      "accuracy :  100.0\n",
      "optional crop  1  : cabbage\n",
      "accuracy :  1.207191e-07\n",
      "optional crop  2  : pineapple\n",
      "accuracy :  3.9925446e-08\n",
      "optional crop  3  : banana\n",
      "accuracy :  5.4108815e-09\n",
      "optional crop  4  : radish\n",
      "accuracy :  2.5812725e-11\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "def dl_predict(new_data_new):\n",
    "    new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(np.array(probs))/4)*100\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)]) \n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]\n",
    "dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304a59a",
   "metadata": {},
   "source": [
    "# Ml DL combined probability predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Most Probable Crop: wheat\n",
      "accuracy :  98.38310470935116\n",
      "optional crop  1  : ashgourd\n",
      "accuracy :  0.31745695918999256\n",
      "optional crop  2  : bittergourd\n",
      "accuracy :  0.25578093807567287\n",
      "optional crop  3  : sesamum\n",
      "accuracy :  0.21932026129485568\n",
      "optional crop  4  : sunflower\n",
      "accuracy :  0.16151806160648363\n"
     ]
    }
   ],
   "source": [
    "def ml_dl_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data_new = new_data\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data_new = new_data\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "#     new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(np.array(probs))/(len(models_with_proba)+len(dl_models_with_proba)))*100\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)])\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "new_data = [\"jammu and kashmir\",60,30,30,6.11,293.36,14.700000]\n",
    "ml_dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Most Probable Crop: rice\n",
      "accuracy :  83.75081642459814\n",
      "optional crop  1  : jowar\n",
      "accuracy :  12.396102468097451\n",
      "optional crop  2  : jute\n",
      "accuracy :  2.834373036403555\n",
      "optional crop  3  : brinjal\n",
      "accuracy :  0.21272720484659802\n",
      "optional crop  4  : maize\n",
      "accuracy :  0.1350565208040666\n"
     ]
    }
   ],
   "source": [
    "new_data = [\"jammu and kashmir\",80,40,40,5.38,516.68,27.866667]\n",
    "ml_dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a6c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Most Probable Crop: onion\n",
      "accuracy :  93.16162295613249\n",
      "optional crop  1  : cabbage\n",
      "accuracy :  1.4030105399046795\n",
      "optional crop  2  : pineapple\n",
      "accuracy :  0.959614198025847\n",
      "optional crop  3  : tomato\n",
      "accuracy :  0.9447513507714371\n",
      "optional crop  4  : beetroot\n",
      "accuracy :  0.751727117026182\n"
     ]
    }
   ],
   "source": [
    "new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]\n",
    "ml_dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512386a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c529eab",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "    probs = (sum(probs)/len(models_with_proba))*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs)])\n",
    "            good[crops[np.argmax(probs)]]= max(probs)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs)])\n",
    "            optional[crops[np.argmax(probs)]] = max(probs)\n",
    "        print(\"accuracy : \", max(probs))\n",
    "        i = np.argmax(probs)\n",
    "        probs[i] = 0.0\n",
    "    return good, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a853735",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "def dl_predict(new_data_new):\n",
    "    new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(probs2)/4)*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)]) \n",
    "            good[crops[np.argmax(probs3)]]= max(probs3)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "            optional[crops[np.argmax(probs3)]] = max(probs3)\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "    return good, optional\n",
    "# new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]\n",
    "# dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a332dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_dl_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data_new = new_data\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data_new = new_data\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "#     new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(np.array(probs))/(len(models_with_proba)+len(dl_models_with_proba)))*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)])\n",
    "            good[crops[np.argmax(probs3)]]= max(probs3)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "            optional[crops[np.argmax(probs3)]] = max(probs3)\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "    return good, optional\n",
    "# new_data = [\"jammu and kashmir\",60,30,30,6.11,293.36,14.700000]\n",
    "# ml_dl_predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f892db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Most Probable Crop: onion\n",
      "accuracy :  93.16162295613249\n",
      "optional crop  1  : cabbage\n",
      "accuracy :  1.4030105399046795\n",
      "optional crop  2  : pineapple\n",
      "accuracy :  0.959614198025847\n",
      "optional crop  3  : tomato\n",
      "accuracy :  0.9447513507714371\n",
      "optional crop  4  : beetroot\n",
      "accuracy :  0.751727117026182\n",
      "dict_items([('onion', 93.16162295613249)])\n",
      "dict_items([('cabbage', 1.4030105399046795), ('pineapple', 0.959614198025847), ('tomato', 0.9447513507714371), ('beetroot', 0.751727117026182)])\n"
     ]
    }
   ],
   "source": [
    "new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]\n",
    "g,p = ml_dl_predict(new_data)\n",
    "print(g.items())\n",
    "print(p.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd16963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.16162295613249\n",
      "cabbage 1.403010539904671\n",
      "pineapple 0.9596141980257991\n",
      "tomato 0.9447513507714371\n",
      "beetroot 0.751727117026182\n"
     ]
    }
   ],
   "source": [
    "print(list(g.values())[0])\n",
    "for k,v in p.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec835ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI\n",
    "import streamlit as st\n",
    "def main(states):\n",
    "    st.title('Crop Recommendation and Protection Management')\n",
    "\n",
    "    # User inputs\n",
    "    st.subheader('Select Inputs:')\n",
    "    states = states  # Placeholder for state dropdown\n",
    "    state_input = st.selectbox('Select State', states)\n",
    "\n",
    "    # Input fields for the remaining parameters (7 inputs)\n",
    "    param_1 = st.number_input('Parameter 1', value=0)\n",
    "    param_2 = st.number_input('Parameter 2', value=0)\n",
    "    param_3 = st.number_input('Parameter 3', value=0)\n",
    "    param_4 = st.number_input('Parameter 4', value=0)\n",
    "    param_5 = st.number_input('Parameter 5', value=0)\n",
    "    param_6 = st.number_input('Parameter 6', value=0)\n",
    "    param_7 = st.number_input('Parameter 7', value=0)\n",
    "\n",
    "    # Button to trigger predictions\n",
    "    if st.button('Predict'):\n",
    "        st.subheader('Predictions')\n",
    "\n",
    "        # Show predictions based on user selection\n",
    "        prediction_type = st.selectbox('Select Prediction Technique', ['ML', 'DL', 'ML-DL', 'All'], index=3)\n",
    "\n",
    "        if prediction_type == 'ML':\n",
    "            st.subheader('ML Predictions')\n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = ml_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "\n",
    "        elif prediction_type == 'DL':\n",
    "            st.subheader('DL Predictions')\n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "                \n",
    "            # Display DL predictions\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "\n",
    "        elif prediction_type == 'ML-DL':\n",
    "            st.subheader('ML-DL Predictions')\n",
    "            # Display ML-DL predictions\n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = ml_dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "        else:\n",
    "            st.subheader('ML Predictions')\n",
    "            # Display ML predictions\n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = ml_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "\n",
    "            st.subheader('DL Predictions')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Display DL predictions\n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "                \n",
    "            # Display DL predictions\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "            st.subheader('ML-DL Predictions')\n",
    "            \n",
    "            \n",
    "            # Display ML-DL predictions\n",
    "            \n",
    "            # Loading animation while predicting\n",
    "            with st.spinner('Predicting...'):\n",
    "                # Perform predictions based on user input\n",
    "                good,optional = ml_dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6, param_7])\n",
    "            st.write('Most Probable Crop:', list(good.keys())[0])\n",
    "            st.write('Accuracy:', list(g.values())[0])\n",
    "            st.write('Optional Crops:')\n",
    "            for k,v in p.items():\n",
    "                st.write(f\"Optional crop: {k}, Accuracy: {v}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5251e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Define hyperparameters for each model for grid search\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "crops = ['apple', 'arecanut', 'ashgourd', 'banana', 'barley', 'beetroot',\n",
    "       'bittergourd', 'blackgram', 'blackpepper', 'bottlegourd',\n",
    "       'brinjal', 'cabbage', 'cardamom', 'carrot', 'cashewnuts',\n",
    "       'cauliflower', 'coffee', 'coriander', 'cotton', 'cucumber',\n",
    "       'drumstick', 'garlic', 'ginger', 'grapes', 'horsegram',\n",
    "       'jackfruit', 'jowar', 'jute', 'ladyfinger', 'maize', 'mango',\n",
    "       'moong', 'onion', 'orange', 'papaya', 'pineapple', 'pomegranate',\n",
    "       'potato', 'pumpkin', 'radish', 'ragi', 'rapeseed', 'rice',\n",
    "       'ridgegourd', 'sesamum', 'soyabean', 'sunflower', 'sweetpotato',\n",
    "       'tapioca', 'tomato', 'turmeric', 'watermelon', 'wheat']\n",
    "states = ['andaman and nicobar islands', 'andhra pradesh',\n",
    "       'arunachal pradesh', 'assam', 'bihar', 'chandigarh',\n",
    "       'chhattisgarh', 'dadra and nagar haveli', 'goa', 'gujarat',\n",
    "       'haryana', 'himachal pradesh', 'jammu and kashmir', 'jharkhand',\n",
    "       'karnataka', 'kerala', 'madhya pradesh', 'maharashtra', 'manipur',\n",
    "       'meghalaya', 'mizoram', 'nagaland', 'odisha', 'puducherry',\n",
    "       'punjab', 'rajasthan', 'sikkim', 'tamil nadu', 'telangana',\n",
    "       'tripura', 'uttar pradesh', 'uttarakhand', 'west bengal']\n",
    "new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "param_grid = {\n",
    "    'LogisticRegression': {'max_iter': [1000]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'DecisionTreeClassifier': {'max_depth': [None, 5, 10, 15]},\n",
    "    'KNeighborsClassifier': {'n_neighbors': [3, 5, 7]},\n",
    "    'GaussianNB': {},  # No hyperparameters for GaussianNB\n",
    "    'RandomForestClassifier': {'n_estimators': [50, 100, 200]},\n",
    "    'VotingClassifier': {},  # Hyperparameters are set inside VotingClassifier definition\n",
    "    'BaggingClassifier': {'n_estimators': [10, 50, 100]},\n",
    "    'AdaBoostClassifier': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]},\n",
    "    'GradientBoostingClassifier': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]}\n",
    "}\n",
    "\n",
    "# List of models with their respective names\n",
    "models = [\n",
    "    ('LogisticRegression', LogisticRegression()),\n",
    "    ('SVM', SVC()),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier()),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('RandomForestClassifier', RandomForestClassifier()),\n",
    "    ('VotingClassifier', VotingClassifier([('Random Forest', RandomForestClassifier()),\n",
    "                                           ('SVM', SVC()),\n",
    "                                           ('Logistic Regression', LogisticRegression(max_iter=1000))])),\n",
    "    ('BaggingClassifier', BaggingClassifier()),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier()),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier())\n",
    "]\n",
    "# List to store models supporting predict_proba\n",
    "models_with_proba = []\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def data_for_cnn(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    return new_data1\n",
    "def data_for_ann(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    return new_data1\n",
    "def data_for_LSTM(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = np.array(new_scaler.transform([new_data1]))\n",
    "    new_data1 = new_data1.reshape(new_data1.shape[0], 1,new_data1.shape[1])\n",
    "    return new_data1\n",
    "def data_for_voting_dl(new_data1):\n",
    "    #new_data1[0] = states.index(new_data1[0])\n",
    "    new_scaler = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/scalerV1.0.pkl')\n",
    "    new_data1 = new_scaler.transform([new_data1])\n",
    "    new_data1 = [new_data1,new_data1]\n",
    "    return new_data1\n",
    "def prediction_from_ann(new_data):\n",
    "    ann_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/ANN_V10.keras\")\n",
    "    pred = ann_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "def prediction_from_cnn(new_data):\n",
    "    cnn_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/CNN_V10.keras\")\n",
    "    pred = cnn_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "def prediction_from_lstm(new_data):\n",
    "    lstm_model = tf.keras.models.load_model(\"C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/LSTM_V11.keras\")\n",
    "    pred = lstm_model.predict(new_data)\n",
    "    return crops[np.argmax(pred,axis = 1)[0]],pred[0]\n",
    "\n",
    "\n",
    "import joblib\n",
    "def create_lstm_model():\n",
    "    LSTM_V10 = Sequential()\n",
    "    LSTM_V10.add(LSTM(256, input_shape=(1, X_train_reshaped.shape[2]), activation='relu'))\n",
    "    #model.add(Dense(128,activation = 'relu'))\n",
    "    LSTM_V10.add(Dense(53, activation='softmax'))  # Adjust output units and activation for multiclass\n",
    "    # Compile the model\n",
    "    LSTM_V10.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return LSTM_V10\n",
    "\n",
    "\n",
    "\n",
    "# Function to create GRU model\n",
    "def create_gru_model():\n",
    "    GRU_V10 = Sequential()\n",
    "    GRU_V10.add(GRU(256, input_shape=(1, X_train_reshaped.shape[2]), kernel_initializer=he_normal(), activation=LeakyReLU(alpha=0.03), return_sequences=True))\n",
    "    GRU_V10.add(GRU(128, activation=LeakyReLU(alpha=0.03)))  # Additional GRU layer\n",
    "    #model2.add(Dense(64, activation=LeakyReLU(alpha=0.03)))\n",
    "    GRU_V10.add(Dense(53, activation='sigmoid'))\n",
    "    GRU_V10.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.03), metrics=['accuracy'])\n",
    "    return GRU_V10\n",
    "def create_ann_model():\n",
    "    ANN_V10 = Sequential()\n",
    "    # Add layers\n",
    "    ANN_V10.add(Dense(128, input_shape=(1, X_train_reshaped.shape[2]), activation='relu'))\n",
    "    ANN_V10.add(Dense(64, activation='relu'))\n",
    "    ANN_V10.add(Dense(32, activation='relu'))\n",
    "    ANN_V10.add(Dense(53, activation='softmax'))  # Softmax for multi-class classification\n",
    "    # Compile the model\n",
    "    ANN_V10.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return ANN_V10\n",
    "voting_classifier = joblib.load('C:/Users/RAMU GOPI/AA-Major Project/all models/DL models/voting_classifier_100epV12.pkl')\n",
    "def prediction_from_voting_dl(new_data):\n",
    "    pred = voting_classifier.predict_proba(new_data)\n",
    "    return crops[np.argmax(pred)],pred[0]\n",
    "\n",
    "\n",
    "# Check and add models supporting predict_proba to the list\n",
    "for name, model in models:\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        models_with_proba.append((name, model))\n",
    "\n",
    "# Access models_with_proba for further use\n",
    "# print(models_with_proba)\n",
    "dl_models_with_proba = [(data_for_ann,prediction_from_ann),(data_for_cnn,prediction_from_cnn),\n",
    "                        (data_for_LSTM,prediction_from_lstm),(data_for_voting_dl,prediction_from_voting_dl)]\n",
    "def ml_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "    probs = (sum(probs)/len(models_with_proba))*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs)])\n",
    "            good[crops[np.argmax(probs)]]= max(probs)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs)])\n",
    "            optional[crops[np.argmax(probs)]] = max(probs)\n",
    "        print(\"accuracy : \", max(probs))\n",
    "        i = np.argmax(probs)\n",
    "        probs[i] = 0.0\n",
    "    return good, optional\n",
    "\n",
    "def dl_predict(new_data_new):\n",
    "    probs = []\n",
    "    new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(np.array(probs))/5)*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)]) \n",
    "            good[crops[np.argmax(probs3)]]= max(probs3)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "            optional[crops[np.argmax(probs3)]] = max(probs3)\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "    return good, optional\n",
    "# new_data = [\"assam\",120,60,65,6.12,2169.32,23.736364]\n",
    "# dl_predict(new_data)\n",
    "\n",
    "\n",
    "def ml_dl_predict(new_data):\n",
    "    #new_data[0]  =states.index(list(new_data)[0])\n",
    "    probs = []\n",
    "    new_data_new = new_data\n",
    "    new_data[0] = states.index(new_data[0])\n",
    "    new_data_new = new_data\n",
    "    new_data = new_scaler.transform([new_data])\n",
    "    new_data = new_data\n",
    "    # Loop through each model and accumulate prediction probabilities\n",
    "    for name, model_ in models_with_proba:\n",
    "        #new_data = ['andaman and nicobar islands',100,40,140,5.86,1925.68,27.0]\n",
    "        model = joblib.load(f'C:/Users/RAMU GOPI/AA-Major Project/all models/ML models/ml_saved_models/hyper/{name}_hyper_22.pkl')\n",
    "        probabilities = model.predict_proba(new_data)[0]\n",
    "        probs.append(probabilities)\n",
    "#     new_data_new[0] = states.index(new_data_new[0])\n",
    "    for create_data,model in dl_models_with_proba:\n",
    "        data = create_data(new_data_new)\n",
    "        probabilities = model(data)\n",
    "        probs.append(list(probabilities[1]))\n",
    "    probs3 = (sum(np.array(probs))/(len(models_with_proba)+len(dl_models_with_proba)))*100\n",
    "    good = {}\n",
    "    optional = {}\n",
    "    for i in range(5):\n",
    "        if i ==0:\n",
    "            print(\"Most Probable Crop:\", crops[np.argmax(probs3)])\n",
    "            good[crops[np.argmax(probs3)]]= max(probs3)\n",
    "        else:\n",
    "            print(\"optional crop \",i,\" :\",crops[np.argmax(probs3)])\n",
    "            optional[crops[np.argmax(probs3)]] = max(probs3)\n",
    "        print(\"accuracy : \", max(probs3))\n",
    "        i = np.argmax(probs3)\n",
    "        probs3[i] = 0\n",
    "    return good, optional\n",
    "# new_data = [\"jammu and kashmir\",60,30,30,6.11,293.36,14.700000]\n",
    "# ml_dl_predict(new_data)\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# Function to fetch weather data\n",
    "def fetch_weather(city_name):\n",
    "    API_KEY = '0146e9f5467b6ada89e5092a83f0d7fb'  \n",
    "    url = f'http://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={API_KEY}&units=metric'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['cod'] == 200:\n",
    "        temperature = data['main']['temp']\n",
    "        return temperature\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Streamlit UI\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "def recommendation_page(states):\n",
    "    st.title('Crop Recommendation')\n",
    "    placeholder1 = st.empty()\n",
    "\n",
    "    # DL predictions\n",
    "    placeholder1.markdown(\"&nbsp;\")\n",
    "    # Sidebar for inputs\n",
    "    st.sidebar.subheader('Select Inputs:')\n",
    "    states = states  # Placeholder for state dropdown\n",
    "    #state_input = \"jammu and kashmir\"\n",
    "    state_input = st.sidebar.selectbox('Select State', states)\n",
    "    param_1 = st.sidebar.text_input('N', value='60')\n",
    "    param_2 = st.sidebar.text_input('P', value='30')\n",
    "    param_3 = st.sidebar.text_input('K', value='30')\n",
    "    param_4 = st.sidebar.text_input('Ph', value='6.11')\n",
    "    param_5 = st.sidebar.text_input('rainfall', value='293.36')\n",
    "    \n",
    "    temperature = np.abs(fetch_weather(state_input))\n",
    "\n",
    "    if temperature is not None:\n",
    "        # Pop-up message for success\n",
    "        st.success(f\"Current temperature in {state_input} is {temperature}C\")\n",
    "        param_6 = st.sidebar.text_input('Temperature', value=f'{temperature}')\n",
    "    else:\n",
    "        # Pop-up message for failure\n",
    "        st.error(f\"Failed to fetch weather data for {state_input}\")\n",
    "        param_6 = st.sidebar.text_input('Temperature', value='{14.7}')\n",
    "    # Convert text inputs to numeric values for predictions\n",
    "    param_1 = float(param_1)\n",
    "    param_2 = float(param_2)\n",
    "    param_3 = float(param_3)\n",
    "    param_4 = float(param_4)\n",
    "    param_5 = float(param_5)\n",
    "    param_6 = float(param_6)\n",
    "    tech = st.selectbox('Select Technique', ['All','ML-DL',\"ML\",\"DL\"])\n",
    "    # Button to trigger predictions\n",
    "    if st.sidebar.button('Predict'):\n",
    "        st.subheader('Predictions')\n",
    "        \n",
    "        # Creating three columns layout for predictions of three models\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        # ML-DL predictions in the third column\n",
    "        if tech in ['All','ML-DL']:\n",
    "            with col1:\n",
    "                st.subheader('ML-DL Predictions')\n",
    "                # Perform predictions based on user input\n",
    "                good, optional = ml_dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6])\n",
    "\n",
    "                # Display ML-DL predictions\n",
    "                st.write(f\"Most Probable Crop:<H7><font color='green'>{list(good.keys())[0]}</font></H7>\", unsafe_allow_html=True)#'Most Probable Crop:', list(good.keys())[0])\n",
    "                st.write('Accuracy:', round(list(good.values())[0],2),\"%\")\n",
    "                st.write(\" \")\n",
    "                st.write('Optional Crops:')\n",
    "                for k, v in optional.items():\n",
    "                    st.write(f\"{k}, Accuracy: {round(v,2)}\",\"%\")\n",
    "        # ML predictions in the first column\n",
    "        if tech in ['All','ML']:\n",
    "            with col2:\n",
    "                st.subheader('ML Predictions')\n",
    "                # Perform predictions based on user input\n",
    "                good, optional = ml_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6])\n",
    "\n",
    "                # Display ML predictions\n",
    "                st.write(f\"Most Probable Crop:<H7><font color='green'>{list(good.keys())[0]}</font></H7>\", unsafe_allow_html=True)\n",
    "                st.write('Accuracy:', round(list(good.values())[0],2),\"%\")\n",
    "                st.write(\" \")\n",
    "                st.write('Optional Crops:')\n",
    "                for k, v in optional.items():\n",
    "                    st.write(f\"{k}, Accuracy: {round(v,2)}\",\"%\")\n",
    "\n",
    "        # DL predictions in the second column\n",
    "        if tech in ['All','DL']:\n",
    "            with col3:\n",
    "                st.subheader('DL Predictions')\n",
    "                # Perform predictions based on user input\n",
    "                good, optional = dl_predict([state_input, param_1, param_2, param_3, param_4, param_5, param_6])\n",
    "\n",
    "                # Display DL predictions\n",
    "                st.write(f\"Most Probable Crop:<H7><font color='green'>{list(good.keys())[0]}</font></H7>\", unsafe_allow_html=True)#list(good.keys())[0])\n",
    "                st.write('Accuracy:', round(list(good.values())[0],2),\"%\")\n",
    "                st.write(\" \")\n",
    "                st.write('Optional Crops:')\n",
    "                for k, v in optional.items():\n",
    "                    st.write(f\"{k}, Accuracy: {round(v)}\",\"%\")\n",
    "#Disease detect\n",
    "diseases = ['Pepper_bell_Bacterial_spot',\n",
    " 'Pepper_bell_healthy',\n",
    " 'Potato_Early_blight',\n",
    " 'Potato_Late_blight',\n",
    " 'Potato_healthy',\n",
    " 'Tomato_Late_blight',\n",
    " 'Tomato_Tomato_mosaic_virus',\n",
    " 'Tomato Leaf Mold',\n",
    " 'Tomato_Bacterial_spot',\n",
    " 'Tomato_Early_blight',\n",
    " 'Tomato_Spider_mites_Two_spotted_spider_mite',\n",
    " 'Tomato_Target_Spot',\n",
    " 'Tomato Tomato_YellowLeaf Curl Virus',\n",
    " 'Tomato_Septoria_leaf_spot',\n",
    "'Tomato_healthy']\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the trained model and pesticide data\n",
    "load_model2 = tf.keras.models.load_model('C:/Users/RAMU GOPI/AA-Major Project/Crop protectio/disease_detect_V10')\n",
    "load_model2.load_weights('C:/Users/RAMU GOPI/AA-Major Project/Crop protectio/disease_detect_weights_V10')\n",
    "pesti = pd.read_csv(\"C:/Users/RAMU GOPI/AA-Major Project/Crop protectio/pesticides.csv\")\n",
    "\n",
    "\n",
    "def predict_disease(image_file,load_model2):\n",
    "    # Load and preprocess the image for prediction\n",
    "    img = image.load_img(image_file, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Rescale to match the training data preprocessing\n",
    "\n",
    "    prediction = load_model2.predict(img_array)\n",
    "\n",
    "    # Decode the prediction\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_class_name = diseases[predicted_class]\n",
    "\n",
    "    # Get pesticide details based on predicted class\n",
    "    pred = dict(pesti[pesti['Disease'] == predicted_class_name])\n",
    "    \n",
    "    return predicted_class_name, pred\n",
    "\n",
    "def protection_page(load_model2):\n",
    "    st.title('Crop Protection Management')\n",
    "\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        st.sidebar.button('Predict')\n",
    "        st.sidebar.image(uploaded_file, caption='Uploaded Image', use_column_width=True)\n",
    "        st.sidebar.image(uploaded_file, caption='Sample Image', use_column_width=True)\n",
    "        predicted_class, pred = predict_disease(uploaded_file,load_model2)\n",
    "\n",
    "        st.write(f\"**<H4>Predicted Disease:** <font color='green'>{predicted_class}</font></H4>\", unsafe_allow_html=True)\n",
    "        st.write(f\"**<H5>Description:**</H5>\\n{pred['Description'].values[0]}\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"**<H5>Symptoms:</H5>**\\n{pred['Symptoms'].values[0]}\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"**<H5>Pest Management (Organic/Non-Organic):</H5>**\\n{pred['Management (Organic/Non-Organic)'].values[0]}\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"**<H5>Refer here:</H5>**\\n{pred['Website Links'].values[0]}\", unsafe_allow_html=True)\n",
    "\n",
    "    else:\n",
    "        new_image_path = 'C:/Users/RAMU GOPI/AA-Major Project/Crop protectio/PlantVillage1/Tomato__Tomato_YellowLeaf__Curl_Virus/0a1e2ed0-619c-43da-8c47-f8000a252954___UF.GRC_YLCV_Lab 03060.jpg'\n",
    "        img = image.load_img(new_image_path, target_size=(256, 256))#Crop protectio\\PlantVillage1\\\n",
    "        st.sidebar.image(img, caption='Sample Image', use_column_width=True)\n",
    "        if st.sidebar.button('Predict'):\n",
    "            st.write(\"No image uploaded. Showing default sample image <H7><font color='red'>Tomato__Tomato_YellowLeaf__Curl_Virus</font></H7>  prediction.\", unsafe_allow_html=True)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array /= 255.0  # Rescale to match the training data preprocessing\n",
    "\n",
    "            prediction = load_model2.predict(img_array)\n",
    "\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            predicted_class_name = diseases[predicted_class]\n",
    "\n",
    "            pred = dict(pesti[pesti['Disease'] == predicted_class_name])\n",
    "\n",
    "            st.write(f\"**<H4>Predicted Disease:** <font color='green'>{predicted_class_name}</font></H4>\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"**<H5>Description:</H5>**\\n{pred['Description'].values[0]}\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"**<H5>Symptoms:</H5>**\\n{pred['Symptoms'].values[0]}\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"**<H5>Pest Management (Organic/Non-Organic):</H5>**\\n{pred['Management (Organic/Non-Organic)'].values[0]}\", unsafe_allow_html=True)\n",
    "            st.markdown(f\"**<H5>Refer here:</H5>**\\n{pred['Website Links'].values[0]}\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "def main(states):\n",
    "    page = st.sidebar.selectbox(\"Choose a page\", (\"Recommendation\", \"Protection\"))\n",
    "\n",
    "    if page == \"Recommendation\":\n",
    "        recommendation_page(states)\n",
    "    else:\n",
    "        protection_page(load_model2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e521a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
